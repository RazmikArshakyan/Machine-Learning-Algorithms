#include <iostream>
#include <numeric>
#include <memory>
#include <vector>
#include <algorithm>
#include <cmath>

struct Node {
    Node(int feature_index, double threshold, int prediction)
        : feature_index{feature_index}, threshold{threshold}, prediction{prediction} {}

    int feature_index;
    double threshold;
    int prediction;
    std::unique_ptr<Node> left_child = nullptr;
    std::unique_ptr<Node> right_child = nullptr;
};

std::unique_ptr<Node> train_decision_tree(const std::vector<std::vector<double>>& features, const std::vector<int>& labels) {
    if (features.empty() || labels.empty() || features.size() != labels.size()) {
        throw std::invalid_argument{"Decision tree cannot be trained for this input data"};
    }

    const int num_features = features[0].size();
    const int num_instances = features.size(); 

    auto node = std::make_unique<Node>(-1, -1, -1);

    // Checking if all labels are the same
    if (std::all_of(labels.begin() + 1, labels.end(), [&](int label) { return label == labels[0]; })) {
        node->prediction = labels[0];
        return node;
    }

    // Calculating Gini impurity (impurity of parent node)
    const int num_class_0 = std::count(labels.begin(), labels.end(), 0);
    const int num_class_1 = num_instances - num_class_0;
    const double parent_impurity = 1.0 - (std::pow(static_cast<double>(num_class_0), 2) / std::pow(num_instances, 2))
        - std::pow(static_cast<double>(num_class_1), 2) / std::pow(num_instances, 2);
    
    double best_gain = 0.0;
    double best_threshold = 0.0;
    int best_feature_index = -1;

    for (int i = 0; i < num_features; ++i) {
        for (size_t instance_index = 0; instance_index < num_instances; ++instance_index) {
            const double threshold = features[instance_index][i];
            std::vector<int> left_labels, right_labels;

            for (size_t j = 0; j < num_instances; ++j) {
                if (features[j][i] <= threshold) {
                    left_labels.push_back(labels[j]);
                } 
                else {
                    right_labels.push_back(labels[j]);
                }
            }

            const int num_left = left_labels.size();
            const int num_right = right_labels.size();

            const int num_left_class_0 = std::count(left_labels.begin(), left_labels.end(), 0);
            const int num_left_class_1 = num_left - num_left_class_0;

            const int num_right_class_0 = std::count(right_labels.begin(), right_labels.end(), 0);
            const int num_right_class_1 = num_right - num_right_class_0;

            const double left_impurity = 1.0 - (std::pow(static_cast<double>(num_left_class_0), 2) / std::pow(num_left, 2))
                - (std::pow(static_cast<double>(num_left_class_1), 2) / std::pow(num_left, 2));

            const double right_impurity = 1.0 - (std::pow(static_cast<double>(num_right_class_0), 2) / std::pow(num_right, 2))
                - (std::pow(static_cast<double>(num_right_class_1), 2) / std::pow(num_right, 2));      

            const double info_gain = parent_impurity - ((static_cast<double>(num_left) / num_instances) * left_impurity)
                - ((static_cast<double>(num_right) / num_instances) * right_impurity);

            if (info_gain > best_gain) {
                best_gain = info_gain;
                best_feature_index = i;
                best_threshold = threshold;
            }     
        }
    }

    std::vector<int> left_labels, right_labels;
    for (size_t j = 0; j < labels.size(); ++j) {
        if (features[j][best_feature_index] <= best_threshold) {
            left_labels.push_back(labels[j]);
        } 
        else {
            right_labels.push_back(labels[j]);
        }
    }
    
    if (!left_labels.empty()) {
        node->left_child = train_decision_tree(features, left_labels);
    }
    if (!right_labels.empty()) {
        node->right_child = train_decision_tree(features, right_labels);
    }

    node->feature_index = best_feature_index;
    node->threshold = best_threshold;

    return node; 
}

int predict(const Node* root, const std::vector<double>& instance) {
    if (!root) {
        return -1;
    }

    while (root->left_child && root->right_child) {
        if (instance[root->feature_index] <= root->threshold) {
            root = root->left_child.get();
        }
        else {
            root = root->right_child.get();
        }
    }
    return root->prediction;
}

// Using test case generated by ChatGPT
int main() {
    std::vector<std::vector<double>> features = {{2, 1},
                                       {3, 1},
                                       {2, 2},
                                       {0, 1},
                                       {1, 3}};
    std::vector<int> labels = {0, 1, 0, 0, 1};

    try {
        auto root = train_decision_tree(features, labels);

        // Predict class label for a new instance
        std::vector<double> instance = {1, 2};
        int predicted_label = predict(root.get(), instance);

        std::cout << "Predicted label: " << predicted_label << std::endl;
    } 
    catch (const std::invalid_argument &e) {
        std::endl(std::cerr << "Error: " << e.what());
    }

    return 0;
}